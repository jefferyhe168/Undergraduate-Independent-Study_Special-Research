{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"QuestionGenerator_practice.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO/TqhHwxor5lnWXOlreTxh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dkp8s1m1l9w5","executionInfo":{"status":"ok","timestamp":1627485049712,"user_tz":-480,"elapsed":9951,"user":{"displayName":"何冠緯","photoUrl":"","userId":"03629691439050504910"}},"outputId":"b00b7be5-b4b0-4e2b-9e91-d4740a5f59e9"},"source":["!pip install transformers\n","!pip install sentencepiece"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","  Downloading transformers-4.9.1-py3-none-any.whl (2.6 MB)\n","\u001b[K     |████████████████████████████████| 2.6 MB 11.3 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 59.3 MB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n","\u001b[K     |████████████████████████████████| 636 kB 61.3 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 61.4 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Collecting huggingface-hub==0.0.12\n","  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.9.1\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 13.6 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.96\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ese08b7alVIk","executionInfo":{"status":"ok","timestamp":1627485050712,"user_tz":-480,"elapsed":1011,"user":{"displayName":"何冠緯","photoUrl":"","userId":"03629691439050504910"}},"outputId":"b35460f5-c87f-407c-e0fc-cbdadb9cf4cd"},"source":["!git clone https://github.com/amontgomerie/question_generator"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Cloning into 'question_generator'...\n","remote: Enumerating objects: 199, done.\u001b[K\n","remote: Counting objects: 100% (87/87), done.\u001b[K\n","remote: Compressing objects: 100% (77/77), done.\u001b[K\n","remote: Total 199 (delta 45), reused 24 (delta 9), pack-reused 112\u001b[K\n","Receiving objects: 100% (199/199), 101.67 KiB | 9.24 MiB/s, done.\n","Resolving deltas: 100% (100/100), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wi-RijrplrwI","executionInfo":{"status":"ok","timestamp":1627485336155,"user_tz":-480,"elapsed":285445,"user":{"displayName":"何冠緯","photoUrl":"","userId":"03629691439050504910"}},"outputId":"0f90e90d-1076-4ec7-d94a-1bd020f0116c"},"source":["# 執行Question Generator\n","!python 'question_generator/run_qg.py' --text_dir '/content/testing.txt'"],"execution_count":3,"outputs":[{"output_type":"stream","text":["2021-07-28 15:10:57.296764: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","Downloading: 100% 25.0/25.0 [00:00<00:00, 23.5kB/s]\n","Downloading: 100% 1.21k/1.21k [00:00<00:00, 1.23MB/s]\n","Downloading: 100% 792k/792k [00:00<00:00, 3.24MB/s]\n","Downloading: 100% 39.0/39.0 [00:00<00:00, 36.0kB/s]\n","Downloading: 100% 121/121 [00:00<00:00, 95.9kB/s]\n","Downloading: 100% 892M/892M [00:23<00:00, 38.5MB/s]\n","Downloading: 100% 49.0/49.0 [00:00<00:00, 47.8kB/s]\n","Downloading: 100% 482/482 [00:00<00:00, 447kB/s]\n","Downloading: 100% 213k/213k [00:00<00:00, 1.33MB/s]\n","Downloading: 100% 112/112 [00:00<00:00, 107kB/s]\n","Downloading: 100% 433M/433M [00:15<00:00, 28.0MB/s]\n","Generating questions...\n","\n","/usr/local/lib/python3.7/dist-packages/transformers/models/t5/tokenization_t5.py:191: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n","  f\"This sequence already has {self.eos_token}. In future versions this behavior may lead to duplicated eos tokens being added.\"\n","Evaluating QA pairs...\n","\n","1) Q: What are the most common tasks that are taught by natural language models?\n","   A: Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. \n","\n","2) Q: What is the largest model that achieves state of the art results on 7 out of 8 tested?\n","   A: 5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero shot setting but still underfits WebText. \n","\n","3) Q: What is the capacity of the language model?\n","   A: The capacity of the language model is essential to the success of zero shot task transfer and increasing it improves performance in a log linear fashion across tasks. \n","\n","4) Q: What are the improvements in the model?\n","   A: Samples from the model reflect these improvements and contain coherent paragraphs of text. \n","\n","5) Q: How does the language model learn to perform tasks?\n","   A: When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. \n","\n","6) Q: What are the findings of this paper?\n","   A: These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations. \n","\n","7) Q: What is the largest model?\n","   A: Our largest model, GPT2, is a 1. \n","\n","8) Q: How many languages are used in the CoQA dataset?\n","   A: 1. 5B \n","      2. 3 (correct)\n","      3. 8 \n","      4. 4 \n","\n","9) Q: how many parameters can be used to model a language?\n","   A: 1. millions \n","      2. 3 \n","      3. 5B \n","      4. 8 (correct)\n","\n","10) Q: How many languages are conditioned on a document plus questions?\n","    A: 1. 7 \n","       2. 4 (correct)\n","       3. 127,000 \n","       4. zero \n","\n"],"name":"stdout"}]}]}